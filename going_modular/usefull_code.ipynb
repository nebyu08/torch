{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531cdef6-7db1-4a45-bfe8-0c1459548a2a",
   "metadata": {},
   "source": [
    "# get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "015c2baf-4145-4e18-a1ad-0f42a145e76b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "495c53a8-1a14-46f1-96a7-ee4c7b1bc67f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from typing import List,Dict,Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8d1ec93-0e43-4e26-8805-37c187975dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the directory already exists\n"
     ]
    }
   ],
   "source": [
    "data=Path(\"data/\")\n",
    "image_path=data/\"pizza_steak_sushi\"\n",
    "\n",
    "#lets make the directory......\n",
    "if image_path.is_dir():\n",
    "    print(\"the directory already exists\")\n",
    "else:\n",
    "    image_path.mkdir(parents=True,exist_ok=True)\n",
    "    print(\"created the directory...\")\n",
    "\n",
    "#lets load the images into that directory   \"\n",
    "request=requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "#image_file=data/\"pizza-steak-sushi\"\n",
    "with open(data/\"pizza_steak_sushi.zip\",\"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a9d61b43-467e-4a7a-b209-d456c997ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets extract the ziped file\n",
    "with zipfile.ZipFile(data/\"pizza_steak_sushi.zip\",\"r\") as zip_file:\n",
    "    zip_file.extractall(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8f666fa-b1ec-42b9-be84-8807b900a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=image_path/\"train\"\n",
    "test_dir=image_path/\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33308443-001d-4891-bb4a-6a754b4d1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the zipped file\n",
    "os.remove(data/\"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5fa8296-c985-4b83-8265-ce4a02f74b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af5b7-2c86-4c20-a8a1-edb4f2c40ee6",
   "metadata": {},
   "source": [
    "# 1 create datasets,data loader and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "431a5e41-4311-4cdc-a47d-9f1dd8456382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "57ecb926-e404-49ab-9765-76e4d2075107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=train_transform,\n",
    ")\n",
    "test_dataset=ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29a45000-1f64-45ed-b2c4-b30973dbf82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names=train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1dae4e42-5afa-45ba-9a02-79a4c524c5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pizza': 0, 'steak': 1, 'sushi': 2}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idx=train_dataset.class_to_idx\n",
    "class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a7da8d3-2e7d-4c85-84bb-27cd4e059319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 225\n",
      "    Root location: data\\pizza_steak_sushi\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 75\n",
      "    Root location: data\\pizza_steak_sushi\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0f7f0ed-497c-44a2-a0c3-23cfae0d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_dataloader=DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader=DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d15b2316-d28b-491f-a95d-5827b135795e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label=next(iter(train_dataloader))\n",
    "image.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee0aef96-5d3e-42ad-9315-0e0c2cab8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "481cf8fe-53a5-4ed9-b293-02db6c4b0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81ca7fd1-6558-445d-bece-5aac49124135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\"\"\"makes training and testing dataloader\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def create_dataloader(train_dir:str,\n",
    "                     test_dir:str,\n",
    "                    transform:transforms.Compose,\n",
    "                     batch_size:int\n",
    "                     ):\n",
    "    \n",
    "    \"\"\"takes in filepath and returns train data loader,test data loader and numbers of classes\"\"\"\n",
    " \n",
    "    train_dataset=ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=transform,\n",
    "        )\n",
    "    \n",
    "    test_dataset=ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    class_names=train_dataset.classes\n",
    "    \n",
    "    train_dataloader=DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    test_dataloader=DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader,test_dataloader,class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1379a9-dffd-4a33-a64c-a79f86b69698",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## **lets test the module we just created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8e17c52-6625-4895-b17d-877404c577e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44033c2d-562e-4543-a5ab-815e4a903b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df75823a-57d6-4da5-ae6c-c83f1c7bec6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "local_trainloader,local_testloader,class_names=data_setup.create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2fd6fc82-3be7-4e47-bd75-326729f7bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x144810f6190>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x144810f6100>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_trainloader,local_testloader,class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0a77-0c5b-492d-9f7b-fa0417809ace",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# lets the save the datasets,data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff41684-78d6-4f0c-baac-f647b02c2f13",
   "metadata": {},
   "source": [
    "# 2 make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4450b443-f43a-47eb-bba2-0eb0db33a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinnyVGG(nn.Module):\n",
    "    \"\"\" makes tunny vgg model \"\"\"\n",
    "    def __init__(self,input_shape:int,\n",
    "                 n_hidden:int,\n",
    "                output_shape:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                             out_channels=n_hidden,\n",
    "                             kernel_size=3,\n",
    "                             stride=1,\n",
    "                             padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2,\n",
    "                        padding=0)\n",
    "        )\n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                      out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            \n",
    "             nn.ReLU(),\n",
    "            \n",
    "             nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "           \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                      stride=2,\n",
    "                      padding=0)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=n_hidden*13*13,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        #print(f\"initial shape: {x.shape}\")\n",
    "        x=self.block1(x)\n",
    "        #print(f\"after block_1: {x.shape}\")\n",
    "        x=self.block2(x)\n",
    "        #print(f\"after block_2: {x.shape}\")\n",
    "        x=self.classifier(x)\n",
    "        #print(f\"final shape:{x.shape} \")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8697d-7e39-4d3f-aba9-8e0f1b381318",
   "metadata": {},
   "source": [
    "# lets save this class into a scrips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4828449-41c0-4228-9d63-c6ddc74651e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinnyVGG(nn.Module):\n",
    "    \"\"\" makes tunny vgg model \"\"\"\n",
    "    def __init__(self,input_shape:int,\n",
    "                 n_hidden:int,\n",
    "                output_shape:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                             out_channels=n_hidden,\n",
    "                             kernel_size=3,\n",
    "                             stride=1,\n",
    "                             padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2,\n",
    "                        padding=0)\n",
    "        )\n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                      out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            \n",
    "             nn.ReLU(),\n",
    "            \n",
    "             nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "           \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                      stride=2,\n",
    "                      padding=0)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=n_hidden*13*13,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        #print(f\"initial shape: {x.shape}\")\n",
    "        x=self.block1(x)\n",
    "        #print(f\"after block_1: {x.shape}\")\n",
    "        x=self.block2(x)\n",
    "        #print(f\"after block_2: {x.shape}\")\n",
    "        x=self.classifier(x)\n",
    "        #print(f\"final shape:{x.shape} \")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a3ffd-c59e-4c22-9b5d-1ade455aa053",
   "metadata": {},
   "source": [
    "# l**ets test our model builder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "94769861-76de-4cc6-881f-e65328a497c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of TinnyVGG(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular.model_builder import TinnyVGG\n",
    "\n",
    "dummy_model=TinnyVGG(\n",
    "    input_shape=1,\n",
    "    output_shape=10,\n",
    "    n_hidden=10\n",
    ")\n",
    "dummy_model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "48e3365c-a4bb-47c2-937d-f1a9d1b5a654",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check for some shape\n",
    "image,label=next(iter(train_dataloader))\n",
    "image.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "546b448a-dd12-4983-814c-90efa097d733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c085073e-d22a-4701-b133-df083b6ea2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of TinnyVGG(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v9=TinnyVGG(input_shape=3,\n",
    "                   n_hidden=10,\n",
    "                   output_shape=len(class_names)).to(device)\n",
    "model_v9.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c65568-5b99-4808-ba69-27b3d83a09a1",
   "metadata": {},
   "source": [
    "# 3 train step,test step and train all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0106fb1f-60e7-4b67-897d-5571126e71d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "              model:nn.Module,\n",
    "              train_data:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              device:torch.device,\n",
    "              )->Tuple[float,float]:\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    model.train()\n",
    "    \n",
    "    for batch,(x,y) in enumerate(train_data):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        pred_logits=model(x)\n",
    "        \n",
    "        preds=torch.argmax(torch.softmax(pred_logits,dim=1),dim=1)\n",
    "        loss=loss_fn(pred_logits,y).item()\n",
    "        \n",
    "        train_loss+=loss\n",
    "        train_acc+=(preds==y).sum().item()/len(train_data)\n",
    "    \n",
    "        #lets zero grad the gradient\n",
    "        optimizer.zero_grad()\n",
    "        #lets backpropagate\n",
    "        loss.backward()\n",
    "        #lets update the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss/=len(train_data)\n",
    "    train_acc/=len(train_data)\n",
    "    \n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e1ce7704-b40a-4149-be36-c48c338c6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model:nn.Module,\n",
    "             test_data:torch.utils.data.DataLoader,\n",
    "             loss_fn:torch.nn,\n",
    "             device:torch.device=\"cpu\")->Tuple[float,float]:\n",
    "    \n",
    "    test_loss,test_acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch,(x,y) in enumerate(test_data):\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "            y_pred_logits=model(x)\n",
    "            \n",
    "            y_pred=torch.argmax(torch.softmax(y_pred_logits,dim=1),dim=1)\n",
    "            loss=loss_fn(y_pred_logits,y).item()\n",
    "            \n",
    "            test_loss+=loss\n",
    "            test_acc=(y==y_pred).sum().item()/len(test_data)\n",
    "    test_loss/=len(test_data)\n",
    "    test_acc/=len(test_data)\n",
    "    return test_loss,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15628167-1bac-4e53-b008-8b4f7aa271ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model:nn.Module,\n",
    "               train_data:torch.utils.data.DataLoader,\n",
    "               test_data:torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn,\n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                epochs:int,\n",
    "               device:torch.device):\n",
    "    #TRAIN MDOEL\n",
    "    results={\"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"test_loss\":[],\n",
    "            \"test_acc\":[]}\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model=model,\n",
    "                                       train_data=train_data,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device\n",
    "                                       )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        \n",
    "    #TEST THE MDODEL\n",
    "    test_loss,test_acc=0,0\n",
    "    test_loss,test_acc=model(model=test_data,\n",
    "                             test_data=test_data,\n",
    "                            loss_fn=loss_fn,\n",
    "                            device=device\n",
    "                            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6862a-ee2e-4d56-b536-661b3eb1af71",
   "metadata": {},
   "source": [
    "# lets save this functions as module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b323b9e1-b4d8-482e-b180-236a18f99c33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List,Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_step(\n",
    "              model:nn.Module,\n",
    "              train_data:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              device:torch.device,\n",
    "              )->Tuple[float,float]:\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    model.train()\n",
    "    \n",
    "    for batch,(x,y) in enumerate(train_data):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        pred_logits=model(x)\n",
    "        \n",
    "        preds=torch.argmax(torch.softmax(pred_logits,dim=1),dim=1)\n",
    "        loss=loss_fn(pred_logits,y)\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "        train_acc+=(preds==y).sum().item()/len(train_data)\n",
    "    \n",
    "        #lets zero grad the gradient\n",
    "        optimizer.zero_grad()\n",
    "        #lets backpropagate\n",
    "        loss.backward()\n",
    "        #lets update the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss/=len(train_data)\n",
    "    train_acc/=len(train_data)\n",
    "    \n",
    "    return train_loss,train_acc\n",
    "    \n",
    "def test_step(model:nn.Module,\n",
    "             test_data:torch.utils.data.DataLoader,\n",
    "             loss_fn:torch.nn,\n",
    "             device:torch.device=\"cpu\")->Tuple[float,float]:\n",
    "    \n",
    "    test_loss,test_acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch,(x,y) in enumerate(test_data):\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "            y_pred_logits=model(x)\n",
    "            \n",
    "            y_pred=torch.argmax(torch.softmax(y_pred_logits,dim=1),dim=1)\n",
    "            loss=loss_fn(y_pred_logits,y).item()\n",
    "            \n",
    "            test_loss+=loss\n",
    "            test_acc=(y==y_pred).sum().item()/len(test_data)\n",
    "    test_loss/=len(test_data)\n",
    "    test_acc/=len(test_data)\n",
    "    return test_loss,test_acc\n",
    "\n",
    "def train_model(model:nn.Module,\n",
    "               train_data:torch.utils.data.DataLoader,\n",
    "               test_data:torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn,\n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                epochs:int,\n",
    "               device:torch.device):\n",
    "    #TRAIN MDOEL\n",
    "    results={\"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"test_loss\":[],\n",
    "            \"test_acc\":[]}\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model=model,\n",
    "                                       train_data=train_data,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device\n",
    "                                       )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        \n",
    "        #TEST THE MDODEL\n",
    "        test_loss,test_acc=0,0\n",
    "        test_loss,test_acc=test_step(model=model,\n",
    "                             test_data=test_data,\n",
    "                            loss_fn=loss_fn,\n",
    "                            device=device\n",
    "                            )\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e0be1be-e22a-4ea1-a95d-071b3804f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.engine import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ae36e90-aea2-4fe1-93a6-27cdc702a715",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3721115479.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[80], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_data=,\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dummy_results=train_model(\n",
    "    model=dummy_model,\n",
    "    train_data=,\n",
    "    test_data=,\n",
    "    l\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08314d-b2fe-4671-847d-17da36af8037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3 save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44b21fd0-315d-49cd-8bf1-d6fd52265fda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model:nn.Module,\n",
    "              target:str,\n",
    "              model_name:str):\n",
    "    \n",
    "    target_path=Path(target)\n",
    "    target_path.mkdir(parents_ok=True,exists=True)\n",
    "    \n",
    "    torch.save(model,target_path)\n",
    "    #assert the externsion of the model\n",
    "    \n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"),\"model extension should be .pt or .pth\"\n",
    "    model_path=target_path/model_name  #created the directory\n",
    "\n",
    "    print(f\"saving the model into {model_path}\")\n",
    "    torch.save(model.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b65f83-b2cd-420a-bc61-eaf597e16772",
   "metadata": {},
   "source": [
    "# lets modularize the saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e41dfe7-edb2-401a-be54-fdc9afbf02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(model:nn.Module,\n",
    "              target:str,\n",
    "              model_name:str):\n",
    "    \n",
    "    target_path=Path(target)\n",
    "    target_path.mkdir(parents=True,exist_ok=True)\n",
    "    \n",
    "    #torch.save(model,target_path)\n",
    "    #assert the externsion of the model\n",
    "    \n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"),\"model extension should be .pt or .pth\"\n",
    "    model_path=target_path/model_name  #created the directory\n",
    "\n",
    "    print(f\"saving the model into {model_path}\")\n",
    "    torch.save(model.state_dict(),f=str(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f9fa6-1b72-48b0-9117-2d9d83cfbd40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fcb9aa93-cf4d-45da-8f24-6b0da7054be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ee133fa6-ff2d-4205-b7f3-3c094d030014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "625da9ef-847b-4260-8d46-a111dc8869ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:15<00:00,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of time taken to train the model is: 15.532730800000081\n",
      "{'train_loss': [1.099340483546257, 1.1030633747577667, 1.0991303473711014, 1.101421520113945, 1.1000436544418335], 'train_acc': [1.21875, 1.125, 1.265625, 1.125, 1.171875], 'test_loss': [1.0827259222666423, 1.0977166891098022, 1.0840776364008586, 1.099848707516988, 1.0856244564056396], 'test_acc': [1.222222222222222, 0.0, 1.222222222222222, 0.0, 1.222222222222222]}\n",
      "saving the model into models\\tinnyVGG_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "from going_modular.data_setup import create_dataloader\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.model_builder import TinnyVGG\n",
    "from going_modular.utils import save_model\n",
    "from timeit import default_timer as timer\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "n_neurons=10\n",
    "\n",
    "#lets define the data loader-->data sets\n",
    "train_dataloader,test_dataloader,class_names=create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#make the model\n",
    "model_v1=TinnyVGG(\n",
    "    input_shape=3,\n",
    "    output_shape=len(class_names),\n",
    "    n_hidden=n_neurons\n",
    ")\n",
    "\n",
    "#lets set up the parameters\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(params=model_v1.parameters(),lr=learning_rate)\n",
    "\n",
    "start_time=timer()\n",
    "#lets train our model\n",
    "epochs=5\n",
    "results=train_model(\n",
    "    model=model_v1,\n",
    "    train_data=train_dataloader,\n",
    "    test_data=test_dataloader,\n",
    "    loss_fn=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")\n",
    "end_time=timer()\n",
    "print(f\"total amount of time taken to train the model is: {end_time-start_time}\")\n",
    "print(results)\n",
    "\n",
    "#lets save the model\n",
    "save_model(model_v1,target=\"models\",model_name=\"tinnyVGG_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753202d-7176-4a35-9e22-c6dc35f20762",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# lets save the training and saving of model as script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2245e9a7-3297-4f41-a37a-ef532ab5a300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e8bf92c5-c9e6-4b2b-9839-f294b80c1877",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/train.py\n",
    "\n",
    "\n",
    "from data_setup import create_dataloader\n",
    "from engine import train_model\n",
    "from model_builder import TinnyVGG\n",
    "from utils import save_model\n",
    "from timeit import default_timer as timer\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "batch_size=32\n",
    "learning_rate=0.01\n",
    "n_neurons=10\n",
    "\n",
    "train_dir=\"data/pizza_steak_sushi/train\"\n",
    "test_dir=\"data/pizza_steak_sushi/test\"\n",
    "\n",
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "#lets define the data loader-->data sets\n",
    "train_dataloader,test_dataloader,class_names=create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#make the model\n",
    "model_v1=TinnyVGG(\n",
    "    input_shape=3,\n",
    "    output_shape=len(class_names),\n",
    "    n_hidden=n_neurons\n",
    ")\n",
    "\n",
    "#lets set up the parameters\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(params=model_v1.parameters(),lr=learning_rate)\n",
    "\n",
    "start_time=timer()\n",
    "#lets train our model\n",
    "epochs=5\n",
    "results=train_model(\n",
    "    model=model_v1,\n",
    "    train_data=train_dataloader,\n",
    "    test_data=test_dataloader,\n",
    "    loss_fn=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")\n",
    "end_time=timer()\n",
    "print(f\"total amount of time taken to train the model is: {end_time-start_time}\")\n",
    "print(results)\n",
    "\n",
    "#lets save the model\n",
    "save_model(model_v1,target=\"models\",model_name=\"tinnyVGG_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e486d5-289a-4f78-89c2-714bff3d67e1",
   "metadata": {},
   "source": [
    "# train model with one line of code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d702a303-8da1-49df-b17f-b976ea062158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total amount of time taken to train the model is: 7.3340724999999996\n",
      "{'train_loss': [1.099340483546257, 1.1030633747577667, 1.0991303473711014, 1.101421520113945, 1.1000436544418335], 'train_acc': [1.21875, 1.125, 1.265625, 1.125, 1.171875], 'test_loss': [1.0827259222666423, 1.0977166891098022, 1.0840776364008586, 1.099848707516988, 1.0856244564056396], 'test_acc': [1.222222222222222, 0.0, 1.222222222222222, 0.0, 1.222222222222222]}\n",
      "saving the model into models\\tinnyVGG_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 20%|##        | 1/5 [00:01<00:05,  1.34s/it]\n",
      " 40%|####      | 2/5 [00:02<00:03,  1.18s/it]\n",
      " 60%|######    | 3/5 [00:03<00:02,  1.15s/it]\n",
      " 80%|########  | 4/5 [00:05<00:01,  1.40s/it]\n",
      "100%|##########| 5/5 [00:07<00:00,  1.63s/it]\n",
      "100%|##########| 5/5 [00:07<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "!python going_modular/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ec2b22-624c-49f0-875b-7ef7781f947a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50484c46-c3e7-4034-8295-cdfd52a05412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
