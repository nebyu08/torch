{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531cdef6-7db1-4a45-bfe8-0c1459548a2a",
   "metadata": {},
   "source": [
    "# get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "495c53a8-1a14-46f1-96a7-ee4c7b1bc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from typing import List,Dict,Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d1ec93-0e43-4e26-8805-37c187975dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the directory already exists\n"
     ]
    }
   ],
   "source": [
    "data=Path(\"data/\")\n",
    "image_path=data/\"pizza_steak_sushi\"\n",
    "\n",
    "#lets make the directory......\n",
    "if image_path.is_dir():\n",
    "    print(\"the directory already exists\")\n",
    "else:\n",
    "    image_path.mkdir(parents=True,exist_ok=True)\n",
    "    print(\"created the directory...\")\n",
    "\n",
    "#lets load the images into that directory   \"\n",
    "request=requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "#image_file=data/\"pizza-steak-sushi\"\n",
    "with open(data/\"pizza_steak_sushi.zip\",\"wb\") as f:\n",
    "    f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d61b43-467e-4a7a-b209-d456c997ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets extract the ziped file\n",
    "with zipfile.ZipFile(data/\"pizza_steak_sushi.zip\",\"r\") as zip_file:\n",
    "    zip_file.extractall(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f666fa-b1ec-42b9-be84-8807b900a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=image_path/\"train\"\n",
    "test_dir=image_path/\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33308443-001d-4891-bb4a-6a754b4d1f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the zipped file\n",
    "os.remove(data/\"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5fa8296-c985-4b83-8265-ce4a02f74b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5af5b7-2c86-4c20-a8a1-edb4f2c40ee6",
   "metadata": {},
   "source": [
    "# 1 create datasets,data loader and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431a5e41-4311-4cdc-a47d-9f1dd8456382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforms\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ecb926-e404-49ab-9765-76e4d2075107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=train_transform,\n",
    ")\n",
    "test_dataset=ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a45000-1f64-45ed-b2c4-b30973dbf82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names=train_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dae4e42-5afa-45ba-9a02-79a4c524c5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pizza': 0, 'steak': 1, 'sushi': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_idx=train_dataset.class_to_idx\n",
    "class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7da8d3-2e7d-4c85-84bb-27cd4e059319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 225\n",
      "    Root location: data\\pizza_steak_sushi\\train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "           )\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 75\n",
      "    Root location: data\\pizza_steak_sushi\\test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0f7f0ed-497c-44a2-a0c3-23cfae0d7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_dataloader=DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader=DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15b2316-d28b-491f-a95d-5827b135795e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label=next(iter(train_dataloader))\n",
    "image.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee0aef96-5d3e-42ad-9315-0e0c2cab8442",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize(size=(64,64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "481cf8fe-53a5-4ed9-b293-02db6c4b0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81ca7fd1-6558-445d-bece-5aac49124135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_setup.py\n",
    "\"\"\"makes training and testing dataloader\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def create_dataloader(train_dir:str,\n",
    "                     test_dir:str,\n",
    "                    transform:transforms.Compose,\n",
    "                     batch_size:int\n",
    "                     ):\n",
    "    \n",
    "    \"\"\"takes in filepath and returns train data loader,test data loader and numbers of classes\"\"\"\n",
    " \n",
    "    train_dataset=ImageFolder(\n",
    "        root=train_dir,\n",
    "        transform=transform,\n",
    "        )\n",
    "    \n",
    "    test_dataset=ImageFolder(\n",
    "        root=test_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    class_names=train_dataset.classes\n",
    "    \n",
    "    train_dataloader=DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    test_dataloader=DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader,test_dataloader,class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1379a9-dffd-4a33-a64c-a79f86b69698",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## **lets test the module we just created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e17c52-6625-4895-b17d-877404c577e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44033c2d-562e-4543-a5ab-815e4a903b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df75823a-57d6-4da5-ae6c-c83f1c7bec6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from going_modular import data_setup\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "local_trainloader,local_testloader,class_names=data_setup.create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fd6fc82-3be7-4e47-bd75-326729f7bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1448309e4c0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1448309e550>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_trainloader,local_testloader,class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f0a77-0c5b-492d-9f7b-fa0417809ace",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# lets the save the datasets,data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff41684-78d6-4f0c-baac-f647b02c2f13",
   "metadata": {},
   "source": [
    "# 2 make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4450b443-f43a-47eb-bba2-0eb0db33a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinnyVGG(nn.Module):\n",
    "    \"\"\" makes tunny vgg model \"\"\"\n",
    "    def __init__(self,input_shape:int,\n",
    "                 n_hidden:int,\n",
    "                output_shape:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                             out_channels=n_hidden,\n",
    "                             kernel_size=3,\n",
    "                             stride=1,\n",
    "                             padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2,\n",
    "                        padding=0)\n",
    "        )\n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                      out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            \n",
    "             nn.ReLU(),\n",
    "            \n",
    "             nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "           \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                      stride=2,\n",
    "                      padding=0)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=n_hidden*13*13,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        #print(f\"initial shape: {x.shape}\")\n",
    "        x=self.block1(x)\n",
    "        #print(f\"after block_1: {x.shape}\")\n",
    "        x=self.block2(x)\n",
    "        #print(f\"after block_2: {x.shape}\")\n",
    "        x=self.classifier(x)\n",
    "        #print(f\"final shape:{x.shape} \")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8697d-7e39-4d3f-aba9-8e0f1b381318",
   "metadata": {},
   "source": [
    "# lets save this class into a scrips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4828449-41c0-4228-9d63-c6ddc74651e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/model_builder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TinnyVGG(nn.Module):\n",
    "    \"\"\" makes tunny vgg model \"\"\"\n",
    "    def __init__(self,input_shape:int,\n",
    "                 n_hidden:int,\n",
    "                output_shape:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                             out_channels=n_hidden,\n",
    "                             kernel_size=3,\n",
    "                             stride=1,\n",
    "                             padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                        stride=2,\n",
    "                        padding=0)\n",
    "        )\n",
    "        self.block2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=n_hidden,\n",
    "                      out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            \n",
    "             nn.ReLU(),\n",
    "            \n",
    "             nn.Conv2d(in_channels=n_hidden,\n",
    "                     out_channels=n_hidden,\n",
    "                     kernel_size=3,\n",
    "                     stride=1,\n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "           \n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                      stride=2,\n",
    "                      padding=0)\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=n_hidden*13*13,\n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        #print(f\"initial shape: {x.shape}\")\n",
    "        x=self.block1(x)\n",
    "        #print(f\"after block_1: {x.shape}\")\n",
    "        x=self.block2(x)\n",
    "        #print(f\"after block_2: {x.shape}\")\n",
    "        x=self.classifier(x)\n",
    "        #print(f\"final shape:{x.shape} \")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a3ffd-c59e-4c22-9b5d-1ade455aa053",
   "metadata": {},
   "source": [
    "# l**ets test our model builder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94769861-76de-4cc6-881f-e65328a497c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of TinnyVGG(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from going_modular.model_builder import TinnyVGG\n",
    "\n",
    "dummy_model=TinnyVGG(\n",
    "    input_shape=1,\n",
    "    output_shape=10,\n",
    "    n_hidden=10\n",
    ")\n",
    "dummy_model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48e3365c-a4bb-47c2-937d-f1a9d1b5a654",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check for some shape\n",
    "image,label=next(iter(train_dataloader))\n",
    "image.shape,label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "546b448a-dd12-4983-814c-90efa097d733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pizza', 'steak', 'sushi']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c085073e-d22a-4701-b133-df083b6ea2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of TinnyVGG(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v9=TinnyVGG(input_shape=3,\n",
    "                   n_hidden=10,\n",
    "                   output_shape=len(class_names)).to(device)\n",
    "model_v9.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c65568-5b99-4808-ba69-27b3d83a09a1",
   "metadata": {},
   "source": [
    "# 3 train step,test step and train all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0106fb1f-60e7-4b67-897d-5571126e71d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_step(\n",
    "              model:nn.Module,\n",
    "              train_data:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              device:torch.device,\n",
    "              )->Tuple[float,float]:\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    model.train()\n",
    "    \n",
    "    for batch,(x,y) in enumerate(train_data):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        pred_logits=model(x)\n",
    "        \n",
    "        preds=torch.argmax(torch.softmax(pred_logits,dim=1),dim=1)\n",
    "        loss=loss_fn(pred_logits,y).item()\n",
    "        \n",
    "        train_loss+=loss\n",
    "        train_acc+=(preds==y).sum().item()/len(train_data)\n",
    "    \n",
    "        #lets zero grad the gradient\n",
    "        optimizer.zero_grad()\n",
    "        #lets backpropagate\n",
    "        loss.backward()\n",
    "        #lets update the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss/=len(train_data)\n",
    "    train_acc/=len(train_data)\n",
    "    \n",
    "    return train_loss,train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1ce7704-b40a-4149-be36-c48c338c6b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model:nn.Module,\n",
    "             test_data:torch.utils.data.DataLoader,\n",
    "             loss_fn:torch.nn,\n",
    "             device:torch.device=\"cpu\")->Tuple[float,float]:\n",
    "    \n",
    "    test_loss,test_acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch,(x,y) in enumerate(test_data):\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "            y_pred_logits=model(x)\n",
    "            \n",
    "            y_pred=torch.argmax(torch.softmax(y_pred_logits,dim=1),dim=1)\n",
    "            loss=loss_fn(y_pred_logits,y).item()\n",
    "            \n",
    "            test_loss+=loss\n",
    "            test_acc=(y==y_pred).sum().item()/len(test_data)\n",
    "    test_loss/=len(test_data)\n",
    "    test_acc/=len(test_data)\n",
    "    return test_loss,test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15628167-1bac-4e53-b008-8b4f7aa271ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model:nn.Module,\n",
    "               train_data:torch.utils.data.DataLoader,\n",
    "               test_data:torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn,\n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                epochs:int,\n",
    "               device:torch.device):\n",
    "    #TRAIN MDOEL\n",
    "    results={\"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"test_loss\":[],\n",
    "            \"test_acc\":[]}\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model=model,\n",
    "                                       train_data=train_data,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device\n",
    "                                       )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        \n",
    "    #TEST THE MDODEL\n",
    "    test_loss,test_acc=0,0\n",
    "    test_loss,test_acc=model(model=test_data,\n",
    "                             test_data=test_data,\n",
    "                            loss_fn=loss_fn,\n",
    "                            device=device\n",
    "                            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6862a-ee2e-4d56-b536-661b3eb1af71",
   "metadata": {},
   "source": [
    "# lets save this functions as module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b323b9e1-b4d8-482e-b180-236a18f99c33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List,Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_step(\n",
    "              model:nn.Module,\n",
    "              train_data:torch.utils.data.DataLoader,\n",
    "              loss_fn:torch.nn,\n",
    "              optimizer:torch.optim.Optimizer,\n",
    "              device:torch.device,\n",
    "              )->Tuple[float,float]:\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    model.train()\n",
    "    \n",
    "    for batch,(x,y) in enumerate(train_data):\n",
    "        x,y=x.to(device),y.to(device)\n",
    "        pred_logits=model(x)\n",
    "        \n",
    "        preds=torch.argmax(torch.softmax(pred_logits,dim=1),dim=1)\n",
    "        loss=loss_fn(pred_logits,y)\n",
    "        \n",
    "        train_loss+=loss.item()\n",
    "        train_acc+=(preds==y).sum().item()/len(train_data)\n",
    "    \n",
    "        #lets zero grad the gradient\n",
    "        optimizer.zero_grad()\n",
    "        #lets backpropagate\n",
    "        loss.backward()\n",
    "        #lets update the parameter\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss/=len(train_data)\n",
    "    train_acc/=len(train_data)\n",
    "    \n",
    "    return train_loss,train_acc\n",
    "    \n",
    "def test_step(model:nn.Module,\n",
    "             test_data:torch.utils.data.DataLoader,\n",
    "             loss_fn:torch.nn,\n",
    "             device:torch.device=\"cpu\")->Tuple[float,float]:\n",
    "    \n",
    "    test_loss,test_acc=0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for batch,(x,y) in enumerate(test_data):\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "            y_pred_logits=model(x)\n",
    "            \n",
    "            y_pred=torch.argmax(torch.softmax(y_pred_logits,dim=1),dim=1)\n",
    "            loss=loss_fn(y_pred_logits,y).item()\n",
    "            \n",
    "            test_loss+=loss\n",
    "            test_acc=(y==y_pred).sum().item()/len(test_data)\n",
    "    test_loss/=len(test_data)\n",
    "    test_acc/=len(test_data)\n",
    "    return test_loss,test_acc\n",
    "\n",
    "def train_model(model:nn.Module,\n",
    "               train_data:torch.utils.data.DataLoader,\n",
    "               test_data:torch.utils.data.DataLoader,\n",
    "               loss_fn:torch.nn,\n",
    "                optimizer:torch.optim.Optimizer,\n",
    "                epochs:int,\n",
    "               device:torch.device):\n",
    "    #TRAIN MDOEL\n",
    "    results={\"train_loss\":[],\n",
    "            \"train_acc\":[],\n",
    "            \"test_loss\":[],\n",
    "            \"test_acc\":[]}\n",
    "    \n",
    "    train_loss,train_acc=0,0\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss,train_acc=train_step(model=model,\n",
    "                                       train_data=train_data,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                        optimizer=optimizer,\n",
    "                                        device=device\n",
    "                                       )\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        \n",
    "    #TEST THE MDODEL\n",
    "    test_loss,test_acc=0,0\n",
    "    test_loss,test_acc=model(model=model,\n",
    "                             test_data=test_data,\n",
    "                            loss_fn=loss_fn,\n",
    "                            device=device\n",
    "                            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e0be1be-e22a-4ea1-a95d-071b3804f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from going_modular.engine import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ae36e90-aea2-4fe1-93a6-27cdc702a715",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3721115479.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[32], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    train_data=,\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dummy_results=train_model(\n",
    "    model=dummy_model,\n",
    "    train_data=,\n",
    "    test_data=,\n",
    "    l\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae08314d-b2fe-4671-847d-17da36af8037",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3 save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44b21fd0-315d-49cd-8bf1-d6fd52265fda",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model:nn.Module,\n",
    "              target:str,\n",
    "              model_name:str):\n",
    "    \n",
    "    target_path=Path(target)\n",
    "    target_path.mkdir(parents_ok=True,exists=True)\n",
    "    \n",
    "    torch.save(model,target_path)\n",
    "    #assert the externsion of the model\n",
    "    \n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"),\"model extension should be .pt or .pth\"\n",
    "    model_path=target_path/model_name  #created the directory\n",
    "\n",
    "    print(f\"saving the model into {model_path}\")\n",
    "    torch.save(model.state_dict(),f=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b65f83-b2cd-420a-bc61-eaf597e16772",
   "metadata": {},
   "source": [
    "# lets modularize the saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e41dfe7-edb2-401a-be54-fdc9afbf02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlin import Path\n",
    "\n",
    "def save_model(model:nn.Module,\n",
    "              target:str,\n",
    "              model_name:str):\n",
    "    \n",
    "    target_path=Path(target)\n",
    "    target_path.mkdir(parents_ok=True,exists=True)\n",
    "    \n",
    "    torch.save(model,target_path)\n",
    "    #assert the externsion of the model\n",
    "    \n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"),\"model extension should be .pt or .pth\"\n",
    "    model_path=target_path/model_name  #created the directory\n",
    "\n",
    "    print(f\"saving the model into {model_path}\")\n",
    "    torch.save(model.state_dict(),f=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f9fa6-1b72-48b0-9117-2d9d83cfbd40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcb9aa93-cf4d-45da-8f24-6b0da7054be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/pizza_steak_sushi/train'),\n",
       " WindowsPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir,test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee133fa6-ff2d-4205-b7f3-3c094d030014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "625da9ef-847b-4260-8d46-a111dc8869ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.67s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#lets train our model\u001b[39;00m\n\u001b[0;32m     28\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_v1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\deep_learning_scratch\\torch\\going_modular\\engine.py:87\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, test_data, loss_fn, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m#TEST THE MDODEL\u001b[39;00m\n\u001b[0;32m     86\u001b[0m test_loss,test_acc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 87\u001b[0m test_loss,test_acc\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\snakey\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\snakey\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'model'"
     ]
    }
   ],
   "source": [
    "from going_modular.data_setup import create_dataloader\n",
    "from going_modular.engine import train_model\n",
    "from going_modular.model_builder import TinnyVGG\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size=32\n",
    "#lets define the data loader-->data sets\n",
    "train_dataloader,test_dataloader,class_names=create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "#make the model\n",
    "model_v1=TinnyVGG(\n",
    "    input_shape=3,\n",
    "    output_shape=len(class_names),\n",
    "    n_hidden=10\n",
    ")\n",
    "\n",
    "#lets set up the parameters\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(params=model_v1.parameters(),lr=0.01)\n",
    "\n",
    "#lets train our model\n",
    "epochs=5\n",
    "train_model(\n",
    "    model=model_v1,\n",
    "    train_data=train_dataloader,\n",
    "    test_data=test_dataloader,\n",
    "    loss_fn=loss,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e135fa-8915-4e85-96a8-101b8a57624a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf92c5-c9e6-4b2b-9839-f294b80c1877",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb8cf72-710a-4c0a-a21e-a1bdc9831491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
